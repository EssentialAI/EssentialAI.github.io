# About me

<figure markdown="1">
![Image title](images/site-images/image-modified.png){ width="200" }
</figure>

<p style="text-align: center;"><b>Naresh Kumar Devulapally</b></p>

<p style="text-align: center;">| <a href="https://www.linkedin.com/in/nareshdevulapally" target = "_blank">Linkedin</a> | <a href="https://www.linkedin.com/in/nareshdevulapally" target = "_blank">Github</a> | <a href="https://www.linkedin.com/in/nareshdevulapally" target = "_blank">Resume</a> |</p>

I am a Graduate Research and Teaching Assistant at [The State University of New York at Buffalo](https://www.buffalo.edu/). My research is focussed on buildling **Multi-Modal AI models**. My recent works on [Multi-Modal Emotion Recognition across ages](https://www.buffalo.edu/) has been submitted to [ACM Multimedia 2023](https://www.acmmm2023.org/). My previous work named PRIMAL that proposed
**PRI**vacy-preserving **M**ulti-modal **A**ttentive **L**earning framework for real-time emotion tracking in conversations has received the First Honorable mention at the [Agrusa L. Student Innovation Competition](https://engineering.buffalo.edu/computer-science-engineering/news-and-events/events/russell-l-agrusa-cse-student-innovation-competition.html) at UB. My research at UB is advised by Professor [Sreyasee Das Bhattacharjee](https://cse.buffalo.edu/~sreyasee/) and Professor [Junsong Yuan](https://cse.buffalo.edu/~jsyuan/index.html).

Before joining UB as an MS CS student, I worked as **Data Scientist-II** at <a href ="https://www.carbynetech.com/" target = "_blank">Carbynetech India</a> where I led multiple teams to deliver several Computer Vision and Machine Learning projects. My projects increased the revenue of the Data Science team by over **30%** and continue to generate revenue to this date.

## Research Projects

<div>
<div style="float:left; margin-top:5px; padding-right: 18px">
<img src="../images/site-images/primal.PNG" alt="Longtail boat in Thailand" width="350"><p style="margin: 0; padding:0; text-align:center">| <a href= "#">GitHub</a> | <a href= "#">GitHub</a> | <a href= "#">GitHub</a> |</p>
</div>
      
<p>
      We propose a PRIvacy-preserving Multimodal Attentive Learning framework (PRIMAL) that derives the person independent normalized facial Action-Unit based features to estimate the participants’ expression and keeps track of their spatio-temporal states and conversation dynamics in context of their surrounding environment to evaluate the speaker emotion. By designing a novel contrastive loss based optimization framework to capture the self- and cross-modal correlation within a learned descriptor, PRIMAL exhibits promise in accurately identifying the emotion state of an individual speaker in group conversations. For interpretability, we identify top-k words in a conversation, facial action-units and keyframe regions which influence the system's decision. The consistent superior performance over other state-of-the-art works in large-scale public datasets, demonstrate the feasibility of our approach.
    </p>
  
  </div>

---

<div>
<div style="float:left; margin-top:5px; padding-right: 18px">
<img src="../images/site-images/acm-prop.png" alt="Longtail boat in Thailand" width="350"><p style="margin: 0; padding:0; text-align:center">| <a href= "#">GitHub</a> | <a href= "#">GitHub</a> | <a href= "#">GitHub</a> |</p>
</div>
      
<p style="margin-top: 0;">
      We propose a PRIvacy-preserving Multimodal Attentive Learning framework (PRIMAL) that derives the person independent normalized facial Action-Unit based features to estimate the participants’ expression and keeps track of their spatio-temporal states and conversation dynamics in context of their surrounding environment to evaluate the speaker emotion. By designing a novel contrastive loss based optimization framework to capture the self- and cross-modal correlation within a learned descriptor, PRIMAL exhibits promise in accurately identifying the emotion state of an individual speaker in group conversations. For interpretability, we identify top-k words in a conversation, facial action-units and keyframe regions which influence the system's decision. The consistent superior performance over other state-of-the-art works in large-scale public datasets, demonstrate the feasibility of our approach.
    </p>
  
  </div>

<hr>

  <div>
<div style="float:left; margin-top:5px; padding-right: 18px">
<img src="../images/site-images/amus.png" alt="Longtail boat in Thailand" width="350"><p style="margin: 0; padding:0; text-align:center">| <a href= "#">GitHub</a> | <a href= "#">GitHub</a> | <a href= "#">GitHub</a> |</p>
</div>
      
<p style="margin-top: 0;">
      We propose a PRIvacy-preserving Multimodal Attentive Learning framework (PRIMAL) that derives the person independent normalized facial Action-Unit based features to estimate the participants’ expression and keeps track of their spatio-temporal states and conversation dynamics in context of their surrounding environment to evaluate the speaker emotion. By designing a novel contrastive loss based optimization framework to capture the self- and cross-modal correlation within a learned descriptor, PRIMAL exhibits promise in accurately identifying the emotion state of an individual speaker in group conversations. For interpretability, we identify top-k words in a conversation, facial action-units and keyframe regions which influence the system's decision. The consistent superior performance over other state-of-the-art works in large-scale public datasets, demonstrate the feasibility of our approach.
    </p>
  
  </div>
